#!/usr/bin/env python3

"""
Script para verificar qu√© modelos tienes disponibles en Ollama
y cu√°les funcionan mejor con Instructor
"""

import subprocess
import json

def check_ollama_models():
    """Verifica los modelos disponibles en Ollama"""
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        if result.returncode == 0:
            print("üîç MODELOS DISPONIBLES EN OLLAMA:")
            print(result.stdout)
            return result.stdout
        else:
            print("‚ùå Error al obtener modelos de Ollama")
            return None
    except FileNotFoundError:
        print("‚ùå Ollama no est√° instalado o no est√° en PATH")
        return None

def get_recommended_models():
    """Lista de modelos recomendados para Instructor"""
    
    models = {
        "ü•á EXCELENTES (Instructor funciona perfecto)": [
            "llama3.1:8b-instruct-q4_k_m",
            "llama3.1:13b-instruct", 
            "mistral:7b-instruct",
            "codellama:13b-instruct",
            "neural-chat:7b-v3.3-q4_k_m"
        ],
        "ü•à BUENOS (Compatibles con configuraci√≥n)": [
            "llama2:13b-chat",
            "vicuna:13b-v1.5",
            "openchat:7b",
            "starling-lm:7b-alpha"
        ],
        "ü•â PROBLEM√ÅTICOS (Pueden tener issues)": [
            "qwen2.5:7b-instruct-q5_k_m",
            "nous-hermes2pro",
            "dolphin-mixtral:8x7b"
        ]
    }
    
    print("\nüìä COMPATIBILIDAD CON INSTRUCTOR:")
    for category, model_list in models.items():
        print(f"\n{category}:")
        for model in model_list:
            print(f"  ‚Ä¢ {model}")
    
    return models

def recommend_best_model(available_models_text):
    """Recomienda el mejor modelo disponible"""
    
    if not available_models_text:
        return None
        
    recommended_models = [
        "llama3.1:8b-instruct-q4_k_m",
        "llama3.1:13b-instruct", 
        "mistral:7b-instruct",
        "codellama:13b-instruct",
        "llama2:13b-chat"
    ]
    
    available_list = available_models_text.lower()
    
    print("\nüéØ RECOMENDACI√ìN:")
    for model in recommended_models:
        if model.lower() in available_list:
            print(f"‚úÖ USAR: {model}")
            print(f"   Raz√≥n: Excelente compatibilidad con Instructor")
            return model
    
    print("‚ùå No se encontraron modelos altamente compatibles")
    print("üí° Sugerencia: Instalar un modelo recomendado")
    
    return None

def show_installation_commands():
    """Muestra comandos para instalar modelos recomendados"""
    
    print("\nüì• INSTALAR MODELOS RECOMENDADOS:")
    commands = [
        "ollama pull llama3.1:8b-instruct-q4_k_m",
        "ollama pull mistral:7b-instruct", 
        "ollama pull codellama:13b-instruct"
    ]
    
    for cmd in commands:
        print(f"  {cmd}")

if __name__ == "__main__":
    print("ü§ñ VERIFICADOR DE MODELOS PARA INSTRUCTOR\n")
    
    # Verificar modelos disponibles
    available = check_ollama_models()
    
    # Mostrar compatibilidad
    get_recommended_models()
    
    # Recomendar mejor modelo
    if available:
        best = recommend_best_model(available)
        if not best:
            show_installation_commands()
    else:
        print("\nüí° OPCIONES SIN OLLAMA:")
        print("  ‚Ä¢ Usar OpenAI: gpt-3.5-turbo")
        print("  ‚Ä¢ Usar OpenAI: gpt-4o-mini")
        print("  ‚Ä¢ Instalar Ollama y un modelo compatible")
